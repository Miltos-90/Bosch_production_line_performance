{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of internal failures in a production line\n",
    "\n",
    "The dataset comes from the [Bosch production line performance competition](https://www.kaggle.com/c/bosch-production-line-performance/), in which we need to predict internal failures using thousands of measurements and tests made for each component along the assembly line. \n",
    "\n",
    "The data for this competition represents measurements of parts as they move through Bosch's production lines. Each part has a unique Id. The goal is to predict which parts will fail quality control (represented by a 'Response' = 1).\n",
    "\n",
    "The dataset contains an extremely large number of anonymized features. Features are named according to a convention that tells you the production line, the station on the line, and a feature number. E.g. L3_S36_F3939 is a feature measured on line 3, station 36, and is feature number 3939.\n",
    "\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import metric\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "\n",
    "import pickle\n",
    "from bayes_opt import BayesianOptimization\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the contents of the zipped files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint (saved at the end of the EDA notebook)\n",
    "file_name = \"./datasets.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "X_train, X_holdout, y_train, y_holdout, skf = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric\n",
    "\n",
    "We need a function to compute the Matthews Correlation Coefficient (MCC) in an efficient way for xgboost. We'll use some numba magic for this, so as to optimise the threshold probability as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 ms ± 16.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "y_prob0 = np.random.rand(1000000)\n",
    "y_prob  = y_prob0 + 0.4 * np.random.rand(1000000) - 0.02\n",
    "y_true  = (y_prob0 > 0.6).astype(int)\n",
    "\n",
    "%timeit metric.eval_mcc(y_true, y_prob)\n",
    "\n",
    "del y_prob0, y_prob, y_true\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold CV\n",
    "\n",
    "We'll use xgboost as the learning algorithm. Let's write a wrapper to perform k-fold CV and return the average validation MCC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   | colsam... |    eta    |   gamma   |   lamda   | max_depth | num_bo... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.2763  \u001b[0m | \u001b[0m 6.122   \u001b[0m | \u001b[0m 0.5761  \u001b[0m | \u001b[0m 0.159   \u001b[0m | \u001b[0m 76.93   \u001b[0m | \u001b[0m 2.953   \u001b[0m | \u001b[0m 11.71   \u001b[0m | \u001b[0m 12.02   \u001b[0m | \u001b[0m 0.6891  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.2893  \u001b[0m | \u001b[95m 2.387   \u001b[0m | \u001b[95m 0.6519  \u001b[0m | \u001b[95m 0.2977  \u001b[0m | \u001b[95m 23.77   \u001b[0m | \u001b[95m 0.8119  \u001b[0m | \u001b[95m 35.13   \u001b[0m | \u001b[95m 65.91   \u001b[0m | \u001b[95m 0.6234  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.3089  \u001b[0m | \u001b[95m 4.662   \u001b[0m | \u001b[95m 0.5533  \u001b[0m | \u001b[95m 0.06849 \u001b[0m | \u001b[95m 90.08   \u001b[0m | \u001b[95m 7.94    \u001b[0m | \u001b[95m 42.83   \u001b[0m | \u001b[95m 83.37   \u001b[0m | \u001b[95m 0.9459  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.2689  \u001b[0m | \u001b[0m 5.773   \u001b[0m | \u001b[0m 0.8662  \u001b[0m | \u001b[0m 0.1553  \u001b[0m | \u001b[0m 2.745   \u001b[0m | \u001b[0m 4.541   \u001b[0m | \u001b[0m 9.74    \u001b[0m | \u001b[0m 83.55   \u001b[0m | \u001b[0m 0.814   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.3226  \u001b[0m | \u001b[95m 5.653   \u001b[0m | \u001b[95m 0.6234  \u001b[0m | \u001b[95m 0.2996  \u001b[0m | \u001b[95m 13.8    \u001b[0m | \u001b[95m 6.154   \u001b[0m | \u001b[95m 26.83   \u001b[0m | \u001b[95m 46.45   \u001b[0m | \u001b[95m 0.8275  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.2902  \u001b[0m | \u001b[0m 3.228   \u001b[0m | \u001b[0m 0.6802  \u001b[0m | \u001b[0m 0.1297  \u001b[0m | \u001b[0m 94.72   \u001b[0m | \u001b[0m 9.187   \u001b[0m | \u001b[0m 41.62   \u001b[0m | \u001b[0m 13.07   \u001b[0m | \u001b[0m 0.9245  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.262   \u001b[0m | \u001b[0m 9.504   \u001b[0m | \u001b[0m 0.863   \u001b[0m | \u001b[0m 0.1703  \u001b[0m | \u001b[0m 96.68   \u001b[0m | \u001b[0m 4.17    \u001b[0m | \u001b[0m 19.35   \u001b[0m | \u001b[0m 11.48   \u001b[0m | \u001b[0m 0.5172  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.2773  \u001b[0m | \u001b[0m 0.5309  \u001b[0m | \u001b[0m 0.5576  \u001b[0m | \u001b[0m 0.05835 \u001b[0m | \u001b[0m 22.64   \u001b[0m | \u001b[0m 5.439   \u001b[0m | \u001b[0m 13.04   \u001b[0m | \u001b[0m 26.14   \u001b[0m | \u001b[0m 0.5673  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.3113  \u001b[0m | \u001b[0m 6.83    \u001b[0m | \u001b[0m 0.7111  \u001b[0m | \u001b[0m 0.2421  \u001b[0m | \u001b[0m 66.88   \u001b[0m | \u001b[0m 4.413   \u001b[0m | \u001b[0m 31.79   \u001b[0m | \u001b[0m 51.55   \u001b[0m | \u001b[0m 0.9335  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.2662  \u001b[0m | \u001b[0m 1.478   \u001b[0m | \u001b[0m 0.9081  \u001b[0m | \u001b[0m 0.1745  \u001b[0m | \u001b[0m 59.25   \u001b[0m | \u001b[0m 6.847   \u001b[0m | \u001b[0m 12.99   \u001b[0m | \u001b[0m 87.82   \u001b[0m | \u001b[0m 0.7169  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.2749  \u001b[0m | \u001b[0m 0.6934  \u001b[0m | \u001b[0m 0.6641  \u001b[0m | \u001b[0m 0.1404  \u001b[0m | \u001b[0m 47.21   \u001b[0m | \u001b[0m 1.795   \u001b[0m | \u001b[0m 12.89   \u001b[0m | \u001b[0m 95.42   \u001b[0m | \u001b[0m 0.7951  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.2736  \u001b[0m | \u001b[0m 1.35    \u001b[0m | \u001b[0m 0.9175  \u001b[0m | \u001b[0m 0.2604  \u001b[0m | \u001b[0m 5.278   \u001b[0m | \u001b[0m 2.037   \u001b[0m | \u001b[0m 44.26   \u001b[0m | \u001b[0m 70.65   \u001b[0m | \u001b[0m 0.6338  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.2941  \u001b[0m | \u001b[0m 7.344   \u001b[0m | \u001b[0m 0.7783  \u001b[0m | \u001b[0m 0.1026  \u001b[0m | \u001b[0m 80.21   \u001b[0m | \u001b[0m 0.5911  \u001b[0m | \u001b[0m 18.87   \u001b[0m | \u001b[0m 28.66   \u001b[0m | \u001b[0m 0.9375  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.2784  \u001b[0m | \u001b[0m 9.094   \u001b[0m | \u001b[0m 0.8044  \u001b[0m | \u001b[0m 0.0788  \u001b[0m | \u001b[0m 95.8    \u001b[0m | \u001b[0m 0.9171  \u001b[0m | \u001b[0m 33.34   \u001b[0m | \u001b[0m 91.26   \u001b[0m | \u001b[0m 0.6529  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.2653  \u001b[0m | \u001b[0m 6.528   \u001b[0m | \u001b[0m 0.9163  \u001b[0m | \u001b[0m 0.1435  \u001b[0m | \u001b[0m 77.14   \u001b[0m | \u001b[0m 8.962   \u001b[0m | \u001b[0m 32.91   \u001b[0m | \u001b[0m 23.6    \u001b[0m | \u001b[0m 0.6687  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.2946  \u001b[0m | \u001b[0m 6.358   \u001b[0m | \u001b[0m 0.7066  \u001b[0m | \u001b[0m 0.1831  \u001b[0m | \u001b[0m 35.23   \u001b[0m | \u001b[0m 6.599   \u001b[0m | \u001b[0m 26.44   \u001b[0m | \u001b[0m 50.91   \u001b[0m | \u001b[0m 0.5562  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.289   \u001b[0m | \u001b[0m 4.125   \u001b[0m | \u001b[0m 0.6229  \u001b[0m | \u001b[0m 0.1367  \u001b[0m | \u001b[0m 97.44   \u001b[0m | \u001b[0m 2.156   \u001b[0m | \u001b[0m 35.73   \u001b[0m | \u001b[0m 12.86   \u001b[0m | \u001b[0m 0.8759  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.27    \u001b[0m | \u001b[0m 7.017   \u001b[0m | \u001b[0m 0.9347  \u001b[0m | \u001b[0m 0.198   \u001b[0m | \u001b[0m 21.73   \u001b[0m | \u001b[0m 7.192   \u001b[0m | \u001b[0m 6.389   \u001b[0m | \u001b[0m 47.68   \u001b[0m | \u001b[0m 0.6266  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.2975  \u001b[0m | \u001b[0m 1.062   \u001b[0m | \u001b[0m 0.6919  \u001b[0m | \u001b[0m 0.2601  \u001b[0m | \u001b[0m 11.04   \u001b[0m | \u001b[0m 9.867   \u001b[0m | \u001b[0m 15.5    \u001b[0m | \u001b[0m 74.93   \u001b[0m | \u001b[0m 0.7974  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.2833  \u001b[0m | \u001b[0m 0.7776  \u001b[0m | \u001b[0m 0.8138  \u001b[0m | \u001b[0m 0.1163  \u001b[0m | \u001b[0m 90.18   \u001b[0m | \u001b[0m 8.183   \u001b[0m | \u001b[0m 22.97   \u001b[0m | \u001b[0m 83.83   \u001b[0m | \u001b[0m 0.589   \u001b[0m |\n",
      "| \u001b[95m 21      \u001b[0m | \u001b[95m 0.3334  \u001b[0m | \u001b[95m 6.459   \u001b[0m | \u001b[95m 0.5462  \u001b[0m | \u001b[95m 0.2506  \u001b[0m | \u001b[95m 36.25   \u001b[0m | \u001b[95m 6.777   \u001b[0m | \u001b[95m 25.78   \u001b[0m | \u001b[95m 49.77   \u001b[0m | \u001b[95m 0.9422  \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.3012  \u001b[0m | \u001b[0m 4.741   \u001b[0m | \u001b[0m 0.7108  \u001b[0m | \u001b[0m 0.1743  \u001b[0m | \u001b[0m 13.35   \u001b[0m | \u001b[0m 7.41    \u001b[0m | \u001b[0m 26.38   \u001b[0m | \u001b[0m 48.17   \u001b[0m | \u001b[0m 0.7132  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 3.441   \u001b[0m | \u001b[0m 0.8176  \u001b[0m | \u001b[0m 0.1366  \u001b[0m | \u001b[0m 14.58   \u001b[0m | \u001b[0m 4.772   \u001b[0m | \u001b[0m 25.93   \u001b[0m | \u001b[0m 46.8    \u001b[0m | \u001b[0m 0.5528  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.279   \u001b[0m | \u001b[0m 3.108   \u001b[0m | \u001b[0m 0.8705  \u001b[0m | \u001b[0m 0.1189  \u001b[0m | \u001b[0m 87.29   \u001b[0m | \u001b[0m 8.282   \u001b[0m | \u001b[0m 41.84   \u001b[0m | \u001b[0m 83.65   \u001b[0m | \u001b[0m 0.6098  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.2937  \u001b[0m | \u001b[0m 6.576   \u001b[0m | \u001b[0m 0.86    \u001b[0m | \u001b[0m 0.2396  \u001b[0m | \u001b[0m 34.55   \u001b[0m | \u001b[0m 6.366   \u001b[0m | \u001b[0m 25.5    \u001b[0m | \u001b[0m 50.53   \u001b[0m | \u001b[0m 0.7603  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.2701  \u001b[0m | \u001b[0m 5.059   \u001b[0m | \u001b[0m 0.8118  \u001b[0m | \u001b[0m 0.09802 \u001b[0m | \u001b[0m 36.69   \u001b[0m | \u001b[0m 8.31    \u001b[0m | \u001b[0m 26.07   \u001b[0m | \u001b[0m 51.84   \u001b[0m | \u001b[0m 0.6583  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.2943  \u001b[0m | \u001b[0m 4.404   \u001b[0m | \u001b[0m 0.6211  \u001b[0m | \u001b[0m 0.2609  \u001b[0m | \u001b[0m 36.23   \u001b[0m | \u001b[0m 7.353   \u001b[0m | \u001b[0m 23.56   \u001b[0m | \u001b[0m 47.98   \u001b[0m | \u001b[0m 0.667   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.2895  \u001b[0m | \u001b[0m 1.508   \u001b[0m | \u001b[0m 0.6033  \u001b[0m | \u001b[0m 0.1134  \u001b[0m | \u001b[0m 12.35   \u001b[0m | \u001b[0m 9.101   \u001b[0m | \u001b[0m 16.4    \u001b[0m | \u001b[0m 75.78   \u001b[0m | \u001b[0m 0.7704  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.3153  \u001b[0m | \u001b[0m 5.752   \u001b[0m | \u001b[0m 0.6329  \u001b[0m | \u001b[0m 0.1538  \u001b[0m | \u001b[0m 14.05   \u001b[0m | \u001b[0m 7.219   \u001b[0m | \u001b[0m 28.18   \u001b[0m | \u001b[0m 45.32   \u001b[0m | \u001b[0m 0.7804  \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.2692  \u001b[0m | \u001b[0m 5.248   \u001b[0m | \u001b[0m 0.9104  \u001b[0m | \u001b[0m 0.1756  \u001b[0m | \u001b[0m 15.03   \u001b[0m | \u001b[0m 5.825   \u001b[0m | \u001b[0m 26.13   \u001b[0m | \u001b[0m 49.58   \u001b[0m | \u001b[0m 0.5196  \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Make parameter set for Tree booster\n",
    "params = {\n",
    "    \"eta\": (0.05, 0.3), \n",
    "    \"gamma\": (0, 100),\n",
    "    \"max_depth\": (5, 50), \n",
    "    \"num_boost_round\": (10, 100), \n",
    "    \"subsample\": (0.5, 0.95), \n",
    "    \"colsample_bytree\": (0.5, 0.95), \n",
    "    \"alpha\": (0, 10), \n",
    "    \"lamda\": (0, 10)} \n",
    "\n",
    "# Function handle\n",
    "f = partial(utils.CV, X_train, y_train, skf)\n",
    "\n",
    "optimizer = BayesianOptimization(f, params, random_state = 111)\n",
    "optimizer.maximize(init_points = 20, n_iter = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the best model on all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dmatrices\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dheld  = xgb.DMatrix(X_holdout, y_holdout.to_numpy())\n",
    "\n",
    "# Scale positive instances\n",
    "sum_neg, sum_pos = np.sum(y_train == 0), np.sum(y_train == 1)\n",
    "\n",
    "# Make parameter dict for xgboost\n",
    "xgb_params = {\"nthread\": -1, \"booster\":\"gbtree\", \"objective\": \"binary:logistic\", \"eval_metric\": \"auc\", \"tree_method\": \"hist\",\n",
    "              \"eta\":              optimizer.max[\"params\"][\"eta\"], \n",
    "              \"gamma\":            optimizer.max[\"params\"][\"gamma\"], \n",
    "              \"max_depth\":        int(optimizer.max[\"params\"][\"max_depth\"]), \n",
    "              \"subsample\":        optimizer.max[\"params\"][\"subsample\"],\n",
    "              \"alpha\":            optimizer.max[\"params\"][\"alpha\"], \n",
    "              \"lambda\":           optimizer.max[\"params\"][\"lamda\"],\n",
    "              \"colsample_bytree\": optimizer.max[\"params\"][\"colsample_bytree\"],\n",
    "             \"scale_pos_weight\" : sum_neg / sum_pos}\n",
    "\n",
    "# Train using the parameters\n",
    "clf = xgb.train(params = xgb_params,\n",
    "                dtrain = dtrain,\n",
    "                feval  = metric.mcc_eval,\n",
    "                evals  = [(dtrain, 'train')],\n",
    "                maximize = True,\n",
    "                verbose_eval = False,\n",
    "                num_boost_round = int(optimizer.max[\"params\"][\"num_boost_round\"]),\n",
    "                early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict on the heldout set and compute the MCC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heldout Set MCC: 0.255\n"
     ]
    }
   ],
   "source": [
    "y_prob = clf.predict(dheld)\n",
    "print(f\"Heldout Set MCC: {round(metric.eval_mcc(y_holdout.to_numpy(), y_prob), 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
